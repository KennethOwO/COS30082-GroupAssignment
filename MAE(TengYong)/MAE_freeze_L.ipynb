{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb174e09",
   "metadata": {},
   "source": [
    "# 1. Imports & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c05d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Transformers version: 4.50.3\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from transformers import ViTMAEModel, get_cosine_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6ee66",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd059362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(r\"D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset\")\n",
    "list_dir  = data_root / \"list\"\n",
    "\n",
    "class Config:\n",
    "    data_root = data_root\n",
    "    list_dir  = list_dir\n",
    "\n",
    "    train_list_path = list_dir / \"train.txt\"\n",
    "    test_list_path  = list_dir / \"test.txt\"\n",
    "    gt_list_path    = list_dir / \"groundtruth.txt\"\n",
    "\n",
    "    model_name = \"facebook/vit-mae-large\"\n",
    "    num_classes = 100\n",
    "\n",
    "    img_size   = 224\n",
    "    batch_size = 16\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    persistent_workers = False\n",
    "\n",
    "    epochs        = 30\n",
    "    lr            = 1e-3\n",
    "    weight_decay  = 1e-4\n",
    "    warmup_ratio  = 0.1\n",
    "    warmup_epochs = 1\n",
    "\n",
    "    seed = 42\n",
    "    # out_dir = Path(\"./test\")\n",
    "    out_dir = Path(\"./runs_mae_freeze_large_NOAUG\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474028d",
   "metadata": {},
   "source": [
    "# 3. Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4815f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f5967",
   "metadata": {},
   "source": [
    "# 4. Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac36d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_AUG = False\n",
    "\n",
    "# ===============================\n",
    "# Define all transforms\n",
    "# ===============================\n",
    "\n",
    "IMAGE_SIZE = 224  # 如果你文件里是别的，请同步修改\n",
    "\n",
    "# Herbarium heavy augmentation\n",
    "train_herbarium_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(45),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Photo light augmentation\n",
    "train_photo_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMAGE_SIZE),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Validation/Test: no augmentation\n",
    "eval_transform = T.Compose([\n",
    "    T.Resize(IMAGE_SIZE + 32),\n",
    "    T.CenterCrop(IMAGE_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ===============================\n",
    "# Unified transform selector\n",
    "# ===============================\n",
    "def pick_transform(rel_path: str, train: bool = True):\n",
    "    \"\"\"\n",
    "    Automatically pick transform based on folder name.\n",
    "    rel_path: dataset path (string), used to detect 'herbarium' or 'photo'.\n",
    "    train: if False → always use eval_transform\n",
    "    \"\"\"\n",
    "    low = rel_path.lower()\n",
    "\n",
    "    # -----------------------\n",
    "    # Validation/test mode: never augment\n",
    "    # -----------------------\n",
    "    if not train:\n",
    "        return eval_transform\n",
    "\n",
    "    # -----------------------\n",
    "    # Training but augmentation disabled\n",
    "    # -----------------------\n",
    "    if not USE_AUG:\n",
    "        return eval_transform\n",
    "\n",
    "    # -----------------------\n",
    "    # Training with augmentation enabled\n",
    "    # -----------------------\n",
    "    if \"herbarium\" in low:\n",
    "        return train_herbarium_transform\n",
    "\n",
    "    if \"photo\" in low:\n",
    "        return train_photo_transform\n",
    "\n",
    "    # Default → treat as Photo dataset\n",
    "    return train_photo_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e063e48",
   "metadata": {},
   "source": [
    "# 5. Loading train/test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec53634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Path Check ===\n",
      "D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset -> True\n",
      "D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset\\list\\train.txt -> True\n",
      "D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset\\list\\test.txt -> True\n",
      "D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset\\list\\groundtruth.txt -> True\n",
      "Line counts -> train=4744, test=207, groundtruth=207\n",
      "[Info] Original label min=12254, max=285398\n",
      "[Info] Number of unique labels=100\n",
      "[Info] Train samples: 4744, Val samples: 207, Unmatched test: 0\n",
      "[Info] Labels have been remapped to range [0, 99]\n"
     ]
    }
   ],
   "source": [
    "def read_groundtruth(gt_path: Path):\n",
    "    mapping = {}\n",
    "    with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                mapping[parts[0]] = int(parts[-1])\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def read_train_list(list_path: Path):\n",
    "    \"\"\"Expect: train.txt lines are '<rel_path> <label>'\"\"\"\n",
    "    samples = []\n",
    "    with open(list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 2:\n",
    "                raise ValueError(f\"Train line has no label: {line}\")\n",
    "            rel_path = parts[0]\n",
    "            label = int(parts[1])\n",
    "            samples.append((rel_path, label))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def read_test_list(list_path: Path):\n",
    "    \"\"\"Expect: test.txt lines are either '<rel_path>' or '<rel_path> <label>'\"\"\"\n",
    "    samples = []\n",
    "    with open(list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            rel_path = parts[0]\n",
    "            label = int(parts[1]) if len(parts) >= 2 else None\n",
    "            samples.append((rel_path, label))\n",
    "    return samples\n",
    "\n",
    "\n",
    "print(\"=== Path Check ===\")\n",
    "print(f\"{Config.data_root} -> {Config.data_root.exists()}\")\n",
    "print(f\"{Config.train_list_path} -> {Config.train_list_path.exists()}\")\n",
    "print(f\"{Config.test_list_path} -> {Config.test_list_path.exists()}\")\n",
    "print(f\"{Config.gt_list_path} -> {Config.gt_list_path.exists()}\")\n",
    "\n",
    "gt_mapping = read_groundtruth(Config.gt_list_path)\n",
    "train_raw = read_train_list(Config.train_list_path)\n",
    "test_raw  = read_test_list(Config.test_list_path)\n",
    "\n",
    "print(f\"Line counts -> train={len(train_raw)}, test={len(test_raw)}, groundtruth={len(gt_mapping)}\")\n",
    "\n",
    "# Build raw samples with original labels\n",
    "train_samples_raw = []\n",
    "val_samples_raw   = []\n",
    "unmatched = 0\n",
    "\n",
    "# Train: labels come directly from train.txt\n",
    "for rel_path, label in train_raw:\n",
    "    full_path = Config.data_root / rel_path\n",
    "    train_samples_raw.append((full_path, label))\n",
    "\n",
    "# Val/Test: prefer label in test.txt, otherwise use groundtruth.txt\n",
    "for rel_path, label in test_raw:\n",
    "    if label is None:\n",
    "        label = gt_mapping.get(rel_path, None)\n",
    "    if label is None:\n",
    "        unmatched += 1\n",
    "        continue\n",
    "    full_path = Config.data_root / rel_path\n",
    "    val_samples_raw.append((full_path, label))\n",
    "\n",
    "all_labels_raw = [lbl for _, lbl in train_samples_raw] + [lbl for _, lbl in val_samples_raw]\n",
    "unique_labels = sorted(set(all_labels_raw))\n",
    "\n",
    "# Map original labels -> contiguous [0, num_classes-1]\n",
    "label_to_idx = {lab: idx for idx, lab in enumerate(unique_labels)}\n",
    "idx_to_label = {idx: lab for lab, idx in label_to_idx.items()}\n",
    "\n",
    "# Apply mapping\n",
    "train_samples = [(path, label_to_idx[lbl]) for path, lbl in train_samples_raw]\n",
    "val_samples   = [(path, label_to_idx[lbl]) for path, lbl in val_samples_raw]\n",
    "\n",
    "Config.num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"[Info] Original label min={min(all_labels_raw)}, max={max(all_labels_raw)}\")\n",
    "print(f\"[Info] Number of unique labels={Config.num_classes}\")\n",
    "print(f\"[Info] Train samples: {len(train_samples)}, Val samples: {len(val_samples)}, Unmatched test: {unmatched}\")\n",
    "print(f\"[Info] Labels have been remapped to range [0, {Config.num_classes - 1}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a681b8c",
   "metadata": {},
   "source": [
    "# 6. Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4566864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 297, Val batches: 13\n"
     ]
    }
   ],
   "source": [
    "class PlantsDataset(Dataset):\n",
    "    def __init__(self, samples, train: bool = True):\n",
    "        \"\"\"\n",
    "        samples: list of (full_path: Path, label_idx: int)\n",
    "        train : True 表示训练集（会根据 USE_AUG 决定要不要 augmentation）\n",
    "                False 表示验证/测试集（永远不用 augmentation）\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # 根据路径和 train / val 选择对应的 transform\n",
    "        # pick_transform 在前面已经定义好了：\n",
    "        #   def pick_transform(rel_path: str, train: bool = True): ...\n",
    "        rel_path = str(img_path)\n",
    "        transform = pick_transform(rel_path, train=self.train)\n",
    "\n",
    "        img = transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# === Datasets ===\n",
    "train_dataset = PlantsDataset(train_samples, train=True)\n",
    "val_dataset   = PlantsDataset(val_samples,   train=False)\n",
    "\n",
    "# === Dataloaders ===\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    pin_memory=Config.pin_memory,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.num_workers,\n",
    "    pin_memory=Config.pin_memory,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6971de",
   "metadata": {},
   "source": [
    "# 7. MAE Classification Model (Frozen Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55489df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone trainable params: 0\n",
      "Head trainable params:     1154148\n",
      "Total params:              304455780\n"
     ]
    }
   ],
   "source": [
    "class PlantMAEClassifier(nn.Module):\n",
    "    def __init__(self, model_name: str, num_classes: int):\n",
    "        super().__init__()\n",
    "        # 1. 加载 MAE backbone（这里会根据 Config.model_name 加载 base 或 large）\n",
    "        self.backbone = ViTMAEModel.from_pretrained(model_name)\n",
    "\n",
    "        # 2. 自动拿到 hidden size（Base=768, Large=1024），不用手写\n",
    "        hidden_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        # 3. 定义我们自己的分类 head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "        # 4. 冻结 backbone，只训练 head\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # ViTMAEModel 输出 last_hidden_state: [B, num_patches+1, hidden_dim]\n",
    "        outputs = self.backbone(pixel_values=pixel_values)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]   # 取 [CLS] token\n",
    "        logits = self.head(cls_token)\n",
    "        return logits\n",
    "\n",
    "# 实例化模型\n",
    "model = PlantMAEClassifier(\n",
    "    model_name=Config.model_name,          # 这里建议已经设为 \"facebook/vit-mae-large\"\n",
    "    num_classes=Config.num_classes,\n",
    ").to(device)\n",
    "\n",
    "# 打印一下参数情况，确认只有 head 在训练\n",
    "backbone_trainable = sum(p.numel() for p in model.backbone.parameters() if p.requires_grad)\n",
    "head_trainable     = sum(p.numel() for p in model.head.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Backbone trainable params: {backbone_trainable}\")\n",
    "print(f\"Head trainable params:     {head_trainable}\")\n",
    "print(f\"Total params:              {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4abc2",
   "metadata": {},
   "source": [
    "# 7. Loss / Optimizer / Scheduler / AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c71d0ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 8910\n",
      "Warmup steps:         297\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 只训练 head（backbone 已经 requires_grad=False）\n",
    "head_params = model.head.parameters()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    head_params,\n",
    "    lr=Config.lr,                    # 比如 1e-3\n",
    "    weight_decay=Config.weight_decay # 比如 1e-4\n",
    ")\n",
    "\n",
    "# 训练总步数 & warmup 步数\n",
    "num_train_steps = max(1, len(train_loader)) * Config.epochs\n",
    "num_warmup_steps = int(Config.warmup_epochs * len(train_loader))\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_train_steps,\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "print(f\"Total training steps: {num_train_steps}\")\n",
    "print(f\"Warmup steps:         {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d38b7",
   "metadata": {},
   "source": [
    "# 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bae48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Training Started ====\n",
      "Saving logs to: runs_mae_freeze_large_NOAUG\\logs\\training_log.txt\n",
      "Best model will be saved to: runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 297/297 [02:07<00:00,  2.34it/s, acc=7.76, loss=4.16]\n",
      "Epoch 1 [Val]: 100%|██████████| 13/13 [00:05<00:00,  2.19it/s, acc=13.5, loss=4.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss=4.1567, Train Acc=7.76% | Val Loss=4.3049, Val Acc=13.53%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 13.53%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 297/297 [01:20<00:00,  3.71it/s, acc=31.1, loss=2.78]\n",
      "Epoch 2 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.35it/s, acc=29.5, loss=3.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss=2.7845, Train Acc=31.13% | Val Loss=3.7514, Val Acc=29.47%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 29.47%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 297/297 [01:20<00:00,  3.68it/s, acc=45.2, loss=2.05]\n",
      "Epoch 3 [Val]: 100%|██████████| 13/13 [00:04<00:00,  3.24it/s, acc=35.3, loss=3.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss=2.0549, Train Acc=45.15% | Val Loss=3.6673, Val Acc=35.27%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 35.27%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 297/297 [01:18<00:00,  3.78it/s, acc=57.6, loss=1.57]\n",
      "Epoch 4 [Val]: 100%|██████████| 13/13 [00:04<00:00,  3.23it/s, acc=39.1, loss=3.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss=1.5679, Train Acc=57.59% | Val Loss=3.5860, Val Acc=39.13%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 39.13%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 297/297 [01:17<00:00,  3.81it/s, acc=64.5, loss=1.26]\n",
      "Epoch 5 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.27it/s, acc=41.5, loss=3.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss=1.2633, Train Acc=64.50% | Val Loss=3.5814, Val Acc=41.55%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 41.55%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 297/297 [01:19<00:00,  3.72it/s, acc=72.1, loss=1.02] \n",
      "Epoch 6 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.29it/s, acc=43.5, loss=3.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss=1.0176, Train Acc=72.07% | Val Loss=3.7166, Val Acc=43.48%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 43.48%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 297/297 [01:22<00:00,  3.58it/s, acc=76.6, loss=0.828]\n",
      "Epoch 7 [Val]: 100%|██████████| 13/13 [00:04<00:00,  2.68it/s, acc=43.5, loss=3.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss=0.8278, Train Acc=76.62% | Val Loss=3.6929, Val Acc=43.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 297/297 [01:20<00:00,  3.67it/s, acc=80.5, loss=0.671]\n",
      "Epoch 8 [Val]: 100%|██████████| 13/13 [00:04<00:00,  3.10it/s, acc=43.5, loss=3.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss=0.6707, Train Acc=80.50% | Val Loss=3.9072, Val Acc=43.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 297/297 [01:17<00:00,  3.83it/s, acc=83.6, loss=0.573]\n",
      "Epoch 9 [Val]: 100%|██████████| 13/13 [00:04<00:00,  3.24it/s, acc=50.2, loss=3.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss=0.5732, Train Acc=83.64% | Val Loss=3.9321, Val Acc=50.24%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 50.24%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|██████████| 297/297 [01:17<00:00,  3.84it/s, acc=86.1, loss=0.468]\n",
      "Epoch 10 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.37it/s, acc=47.8, loss=4.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss=0.4678, Train Acc=86.07% | Val Loss=4.2146, Val Acc=47.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Train]: 100%|██████████| 297/297 [01:16<00:00,  3.91it/s, acc=88.4, loss=0.394]\n",
      "Epoch 11 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.31it/s, acc=46.4, loss=4.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss=0.3942, Train Acc=88.39% | Val Loss=4.5269, Val Acc=46.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Train]: 100%|██████████| 297/297 [01:15<00:00,  3.92it/s, acc=89.9, loss=0.338]\n",
      "Epoch 12 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.38it/s, acc=48.3, loss=4.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss=0.3379, Train Acc=89.92% | Val Loss=4.3311, Val Acc=48.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Train]: 100%|██████████| 297/297 [01:15<00:00,  3.93it/s, acc=91.8, loss=0.289]\n",
      "Epoch 13 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.38it/s, acc=49.3, loss=4.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss=0.2885, Train Acc=91.84% | Val Loss=4.3326, Val Acc=49.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Train]: 100%|██████████| 297/297 [01:15<00:00,  3.92it/s, acc=92.8, loss=0.241]\n",
      "Epoch 14 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.31it/s, acc=48.8, loss=4.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss=0.2408, Train Acc=92.83% | Val Loss=4.5833, Val Acc=48.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Train]: 100%|██████████| 297/297 [01:15<00:00,  3.92it/s, acc=94.2, loss=0.209]\n",
      "Epoch 15 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.42it/s, acc=51.2, loss=4.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss=0.2095, Train Acc=94.20% | Val Loss=4.3169, Val Acc=51.21%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 51.21%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Train]: 100%|██████████| 297/297 [01:15<00:00,  3.94it/s, acc=94.9, loss=0.19] \n",
      "Epoch 16 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.29it/s, acc=47.3, loss=4.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Train Loss=0.1900, Train Acc=94.86% | Val Loss=4.6111, Val Acc=47.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Train]: 100%|██████████| 297/297 [01:16<00:00,  3.89it/s, acc=95.2, loss=0.165]\n",
      "Epoch 17 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.41it/s, acc=49.8, loss=4.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Train Loss=0.1647, Train Acc=95.24% | Val Loss=4.5777, Val Acc=49.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Train]: 100%|██████████| 297/297 [01:16<00:00,  3.90it/s, acc=96.6, loss=0.137]\n",
      "Epoch 18 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.46it/s, acc=53.6, loss=4.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Train Loss=0.1369, Train Acc=96.56% | Val Loss=4.5825, Val Acc=53.62%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 53.62%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Train]: 100%|██████████| 297/297 [01:16<00:00,  3.89it/s, acc=96.2, loss=0.131]\n",
      "Epoch 19 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.43it/s, acc=50.7, loss=4.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Train Loss=0.1309, Train Acc=96.25% | Val Loss=4.4923, Val Acc=50.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Train]: 100%|██████████| 297/297 [01:15<00:00,  3.93it/s, acc=97, loss=0.118]  \n",
      "Epoch 20 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.42it/s, acc=48.3, loss=4.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Train Loss=0.1179, Train Acc=97.05% | Val Loss=4.7324, Val Acc=48.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Train]: 100%|██████████| 297/297 [01:15<00:00,  3.92it/s, acc=97.6, loss=0.102] \n",
      "Epoch 21 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.47it/s, acc=51.2, loss=4.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Train Loss=0.1018, Train Acc=97.64% | Val Loss=4.7247, Val Acc=51.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [Train]: 100%|██████████| 297/297 [01:14<00:00,  3.96it/s, acc=97.7, loss=0.0952]\n",
      "Epoch 22 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.34it/s, acc=51.2, loss=4.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Train Loss=0.0952, Train Acc=97.66% | Val Loss=4.7286, Val Acc=51.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [Train]: 100%|██████████| 297/297 [01:14<00:00,  3.97it/s, acc=98.1, loss=0.0836]\n",
      "Epoch 23 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.49it/s, acc=50.7, loss=4.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Train Loss=0.0836, Train Acc=98.15% | Val Loss=4.8209, Val Acc=50.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 [Train]: 100%|██████████| 297/297 [01:18<00:00,  3.81it/s, acc=98.5, loss=0.0775]\n",
      "Epoch 24 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.25it/s, acc=51.7, loss=4.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Train Loss=0.0775, Train Acc=98.48% | Val Loss=4.8297, Val Acc=51.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 [Train]: 100%|██████████| 297/297 [01:15<00:00,  3.93it/s, acc=98.1, loss=0.0819]\n",
      "Epoch 25 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.44it/s, acc=51.2, loss=4.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Train Loss=0.0819, Train Acc=98.10% | Val Loss=4.7199, Val Acc=51.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 [Train]: 100%|██████████| 297/297 [01:19<00:00,  3.74it/s, acc=98.7, loss=0.0685]\n",
      "Epoch 26 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.37it/s, acc=49.8, loss=4.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Train Loss=0.0685, Train Acc=98.67% | Val Loss=4.7905, Val Acc=49.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 [Train]: 100%|██████████| 297/297 [01:14<00:00,  3.97it/s, acc=98.6, loss=0.0692]\n",
      "Epoch 27 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.47it/s, acc=52.7, loss=4.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Train Loss=0.0692, Train Acc=98.61% | Val Loss=4.8882, Val Acc=52.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 [Train]: 100%|██████████| 297/297 [01:14<00:00,  3.97it/s, acc=98.5, loss=0.0699]\n",
      "Epoch 28 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.35it/s, acc=54.1, loss=4.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Train Loss=0.0699, Train Acc=98.55% | Val Loss=4.7245, Val Acc=54.11%\n",
      "--> New Best Model Saved at runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth (Val Acc: 54.11%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 [Train]: 100%|██████████| 297/297 [01:14<00:00,  3.99it/s, acc=98.9, loss=0.0604]\n",
      "Epoch 29 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.46it/s, acc=52.7, loss=4.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Train Loss=0.0604, Train Acc=98.92% | Val Loss=4.6945, Val Acc=52.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 [Train]: 100%|██████████| 297/297 [01:14<00:00,  3.96it/s, acc=98.7, loss=0.0612]\n",
      "Epoch 30 [Val]: 100%|██████████| 13/13 [00:03<00:00,  3.45it/s, acc=52.7, loss=4.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Train Loss=0.0612, Train Acc=98.69% | Val Loss=4.6958, Val Acc=52.66%\n",
      "==== Training Completed ====\n",
      "Best Validation Accuracy: 54.11%\n",
      "Best Model Path: runs_mae_freeze_large_NOAUG\\models\\mae_frozen_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create logs directory\n",
    "logs_dir = Config.out_dir / \"logs\"\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create models directory (only for BEST model)\n",
    "models_dir = Config.out_dir / \"models\"\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Fixed log file name (NO timestamp)\n",
    "log_path = logs_dir / \"training_log.txt\"\n",
    "\n",
    "def LogWrite(text):\n",
    "    \"\"\"Helper function to write logs to file + print on screen.\"\"\"\n",
    "    print(text)\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", enabled=device.type == \"cuda\"):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix(loss=running_loss/total, acc=correct/total*100)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", enabled=device.type == \"cuda\"):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix(loss=running_loss/total, acc=correct/total*100)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# ========== Training Loop ==========\n",
    "best_val_acc = 0\n",
    "best_model_path = models_dir / \"mae_frozen_best.pth\"\n",
    "\n",
    "LogWrite(\"==== Training Started ====\")\n",
    "LogWrite(f\"Saving logs to: {log_path}\")\n",
    "LogWrite(f\"Best model will be saved to: {best_model_path}\\n\")\n",
    "\n",
    "for epoch in range(Config.epochs):\n",
    "    train_loss, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_acc = validate(epoch)\n",
    "\n",
    "    LogWrite(\n",
    "        f\"[Epoch {epoch+1}] \"\n",
    "        f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}% | \"\n",
    "        f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # ===== Save ONLY best model =====\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        LogWrite(\n",
    "            f\"--> New Best Model Saved at {best_model_path} \"\n",
    "            f\"(Val Acc: {val_acc:.2f}%)\\n\"\n",
    "        )\n",
    "\n",
    "LogWrite(\"==== Training Completed ====\")\n",
    "LogWrite(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "LogWrite(f\"Best Model Path: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484cb08",
   "metadata": {},
   "source": [
    "# 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2878f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving evaluation results to: runs_mae_freeze_large_NOAUG\\evaluation\n",
      "\n",
      "[Overall] Top-1 Accuracy: 52.66%\n",
      "[Overall] Top-5 Accuracy: 67.15%\n",
      "\n",
      "[Saved] Classification report → runs_mae_freeze_large_NOAUG\\evaluation\\classification_report.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] Confusion matrix image → runs_mae_freeze_large_NOAUG\\evaluation\\confusion_matrix.png\n",
      "[Saved] Per-class CSV → runs_mae_freeze_large_NOAUG\\evaluation\\per_class_metrics.csv\n",
      "\n",
      "[✓] All evaluation results (including Top-1 & Top-5) saved successfully.\n",
      "\n",
      "[INFO] Saving with/without-pairs results to: runs_mae_freeze_large_NOAUG\\evaluation\n",
      "#classes in WITH-PAIRS list   : 60\n",
      "#classes in WITHOUT-PAIRS list: 40\n",
      "\n",
      "=== WITH-PAIRS / WITHOUT-PAIRS Result on Validation Set ===\n",
      "Samples in WITH-PAIRS group   : 153\n",
      "Samples in WITHOUT-PAIRS group: 54\n",
      "\n",
      "Group: WITH PAIRS\n",
      "  Top-1 Accuracy: 71.24%\n",
      "  Top-5 Accuracy: 89.54%\n",
      "\n",
      "Group: WITHOUT PAIRS\n",
      "  Top-1 Accuracy: 0.00%\n",
      "  Top-5 Accuracy: 3.70%\n",
      "\n",
      "[Saved] With/without-pairs results → runs_mae_freeze_large_NOAUG\\evaluation\\val_with_without_pairs_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# === Folder to save results ===\n",
    "eval_dir = Config.out_dir / \"evaluation\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Saving evaluation results to: {eval_dir}\")\n",
    "\n",
    "\n",
    "# === helper: top-k accuracy ===\n",
    "def topk_acc_from_topk_array(y_true_group, topk_array):\n",
    "    \"\"\"\n",
    "    y_true_group : [N]\n",
    "    topk_array   : [N, K]，每行是该样本 top-K 预测的类别 index\n",
    "    返回: 百分比 (0-100)\n",
    "    \"\"\"\n",
    "    if len(y_true_group) == 0:\n",
    "        return None\n",
    "    correct = np.any(topk_array == y_true_group[:, None], axis=1)\n",
    "    return correct.mean() * 100.0\n",
    "\n",
    "\n",
    "# === Collect predictions (Top-1 + Top-5) ===\n",
    "@torch.no_grad()\n",
    "def collect_preds(loader, k=5):\n",
    "    model.eval()\n",
    "    all_top1, all_topk, all_labels = [], [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        logits = model(imgs)\n",
    "\n",
    "        top1 = torch.argmax(logits, dim=1)           # [B]\n",
    "        topk = torch.topk(logits, k=k, dim=1).indices  # [B, k]\n",
    "\n",
    "        all_top1.append(top1.cpu().numpy())\n",
    "        all_topk.append(topk.cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "    y_pred_top1 = np.concatenate(all_top1)      # [N]\n",
    "    y_pred_topk = np.concatenate(all_topk)      # [N, k]\n",
    "    y_true      = np.concatenate(all_labels)    # [N]\n",
    "    return y_pred_top1, y_pred_topk, y_true\n",
    "\n",
    "\n",
    "y_pred, y_top5, y_true = collect_preds(val_loader, k=5)\n",
    "\n",
    "# === Overall Top-1 & Top-5 ===\n",
    "overall_top1 = accuracy_score(y_true, y_pred) * 100.0\n",
    "overall_top5 = topk_acc_from_topk_array(y_true, y_top5)\n",
    "\n",
    "print(f\"\\n[Overall] Top-1 Accuracy: {overall_top1:.2f}%\")\n",
    "print(f\"[Overall] Top-5 Accuracy: {overall_top5:.2f}%\\n\")\n",
    "\n",
    "\n",
    "# === 1. Save Classification Report ===\n",
    "report_str = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    digits=4,\n",
    "    target_names=[str(idx_to_label[i]) for i in range(len(idx_to_label))]\n",
    ")\n",
    "\n",
    "report_path = eval_dir / \"classification_report.txt\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Overall Top-1 Accuracy: {overall_top1:.4f}%\\n\")\n",
    "    f.write(f\"Overall Top-5 Accuracy: {overall_top5:.4f}%\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(report_str)\n",
    "\n",
    "print(f\"[Saved] Classification report → {report_path}\")\n",
    "\n",
    "\n",
    "# === 2. Save Confusion Matrix Plot (Top-1) ===\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(idx_to_label))))\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix (counts)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = eval_dir / \"confusion_matrix.png\"\n",
    "plt.savefig(cm_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"[Saved] Confusion matrix image → {cm_path}\")\n",
    "\n",
    "\n",
    "# === 3. Save Per-Class Metrics as CSV (from Top-1) ===\n",
    "report_dict = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    digits=4,\n",
    "    target_names=[str(idx_to_label[i]) for i in range(len(idx_to_label))],\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame(report_dict).transpose()\n",
    "metrics_path = eval_dir / \"per_class_metrics.csv\"\n",
    "metrics_df.to_csv(metrics_path, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"[Saved] Per-class CSV → {metrics_path}\")\n",
    "\n",
    "print(\"\\n[✓] All evaluation results (including Top-1 & Top-5) saved successfully.\")\n",
    "\n",
    "# ====== Step 9 (extra): WITH / WITHOUT PAIRS 分组分析并保存到 evaluation 文件夹 ======\n",
    "\n",
    "# 0. 确保 evaluation 目录存在（和前面 Step 9 保存 report 的目录一致）\n",
    "eval_dir = Config.out_dir / \"evaluation\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n[INFO] Saving with/without-pairs results to: {eval_dir}\")\n",
    "\n",
    "# 1. 设置 with / without pairs 的 class list 文件路径\n",
    "#    确保这两个文件在当前工作目录下，或者改成你的完整路径\n",
    "WITH_PAIRS_FILE = list_dir / \"class_with_pairs.txt\"\n",
    "WITHOUT_PAIRS_FILE = list_dir / \"class_without_pairs.txt\"\n",
    "\n",
    "# 2. 读入原始 class ID（每行一个）\n",
    "with open(WITH_PAIRS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    with_pairs_ids = {line.strip() for line in f if line.strip()}\n",
    "\n",
    "with open(WITHOUT_PAIRS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    without_pairs_ids = {line.strip() for line in f if line.strip()}\n",
    "\n",
    "print(f\"#classes in WITH-PAIRS list   : {len(with_pairs_ids)}\")\n",
    "print(f\"#classes in WITHOUT-PAIRS list: {len(without_pairs_ids)}\")\n",
    "\n",
    "# 3. 把 y_true 里的「类别 index」转换成「原始 class ID 字符串」\n",
    "#    假设 idx_to_label[i] 就是原始 ID（数字或字符串），统一转成 str 来对比\n",
    "y_true_class_ids = np.array([str(idx_to_label[int(c)]) for c in y_true])\n",
    "\n",
    "# 4. 根据 class ID mask 出 with-pair / without-pair 的样本\n",
    "mask_with_pairs = np.isin(y_true_class_ids, list(with_pairs_ids))\n",
    "mask_without_pairs = np.isin(y_true_class_ids, list(without_pairs_ids))\n",
    "\n",
    "y_true_with = y_true[mask_with_pairs]\n",
    "y_pred_with = y_pred[mask_with_pairs]\n",
    "y_top5_with = y_top5[mask_with_pairs]\n",
    "\n",
    "y_true_without = y_true[mask_without_pairs]\n",
    "y_pred_without = y_pred[mask_without_pairs]\n",
    "y_top5_without = y_top5[mask_without_pairs]\n",
    "\n",
    "print(\"\\n=== WITH-PAIRS / WITHOUT-PAIRS Result on Validation Set ===\")\n",
    "print(f\"Samples in WITH-PAIRS group   : {len(y_true_with)}\")\n",
    "print(f\"Samples in WITHOUT-PAIRS group: {len(y_true_without)}\")\n",
    "\n",
    "# 防止某个 group 为空\n",
    "if len(y_true_with) > 0:\n",
    "    acc_with_top1 = accuracy_score(y_true_with, y_pred_with) * 100.0\n",
    "    acc_with_top5 = topk_acc_from_topk_array(y_true_with, y_top5_with)\n",
    "    print(f\"\\nGroup: WITH PAIRS\")\n",
    "    print(f\"  Top-1 Accuracy: {acc_with_top1:.2f}%\")\n",
    "    print(f\"  Top-5 Accuracy: {acc_with_top5:.2f}%\")\n",
    "else:\n",
    "    acc_with_top1 = None\n",
    "    acc_with_top5 = None\n",
    "    print(\"\\nGroup: WITH PAIRS\")\n",
    "    print(\"  (No samples from WITH-PAIRS class IDs found in this validation set.)\")\n",
    "\n",
    "if len(y_true_without) > 0:\n",
    "    acc_without_top1 = accuracy_score(y_true_without, y_pred_without) * 100.0\n",
    "    acc_without_top5 = topk_acc_from_topk_array(y_true_without, y_top5_without)\n",
    "    print(f\"\\nGroup: WITHOUT PAIRS\")\n",
    "    print(f\"  Top-1 Accuracy: {acc_without_top1:.2f}%\")\n",
    "    print(f\"  Top-5 Accuracy: {acc_without_top5:.2f}%\")\n",
    "else:\n",
    "    acc_without_top1 = None\n",
    "    acc_without_top5 = None\n",
    "    print(\"\\nGroup: WITHOUT PAIRS\")\n",
    "    print(\"  (No samples from WITHOUT-PAIRS class IDs found in this validation set.)\")\n",
    "\n",
    "# 5. 把 with / without 的结果也存到当前 run 的 evaluation 目录里\n",
    "out_path = eval_dir / \"val_with_without_pairs_results.txt\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"WITH-PAIRS / WITHOUT-PAIRS Result on Validation Set\\n\")\n",
    "    f.write(f\"Samples in WITH-PAIRS group   : {len(y_true_with)}\\n\")\n",
    "    f.write(f\"Samples in WITHOUT-PAIRS group: {len(y_true_without)}\\n\\n\")\n",
    "\n",
    "    if acc_with_top1 is not None:\n",
    "        f.write(f\"WITH PAIRS Top-1 Accuracy   : {acc_with_top1:.4f}%\\n\")\n",
    "        f.write(f\"WITH PAIRS Top-5 Accuracy   : {acc_with_top5:.4f}%\\n\")\n",
    "    else:\n",
    "        f.write(\"WITH PAIRS: no samples in this val set\\n\")\n",
    "\n",
    "    if acc_without_top1 is not None:\n",
    "        f.write(f\"WITHOUT PAIRS Top-1 Accuracy: {acc_without_top1:.4f}%\\n\")\n",
    "        f.write(f\"WITHOUT PAIRS Top-5 Accuracy: {acc_without_top5:.4f}%\\n\")\n",
    "    else:\n",
    "        f.write(\"WITHOUT PAIRS: no samples in this val set\\n\")\n",
    "\n",
    "print(f\"\\n[Saved] With/without-pairs results → {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
