{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c915adc",
   "metadata": {},
   "source": [
    "# Herbarium vs Field Classification with **facebook/vit-mae-large** (MAE)\n",
    "\n",
    "This notebook fine-tunes the MAE encoder from `facebook/vit-mae-large` as a classifier using your dataset lists:\n",
    "\n",
    "- **Train**: `train.txt` (each line: `relative/image/path label_id`)\n",
    "- **Valid**: constructed from `test.txt` (each line: `relative/image/path`, no label) paired with `groundtruth.txt` (`relative/image/path label_id`)\n",
    "\n",
    "Domain-specific augmentations:\n",
    "- **Herbarium (heavy)**: `RandomResizedCrop(224, scale=(0.8,1.0))`, `ColorJitter(0.4,0.4,0.4,0.2)`, `RandomHorizontalFlip`, `RandomVerticalFlip`, `RandomRotation(45)`\n",
    "- **Photo (light)**: `RandomResizedCrop(224)`, `RandomHorizontalFlip`\n",
    "\n",
    "> Set `DATASET_ROOT` to the folder **containing** the paths referenced by the txt files (typically a folder that contains `train/`, `test/`, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a9922",
   "metadata": {},
   "source": [
    "# Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e389fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies (uncomment):\n",
    "# %pip install -U torch torchvision transformers tqdm scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a0ac29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, math, json, random, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from transformers import AutoImageProcessor, ViTMAEModel, get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c392ec4",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ebd6da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Path Check ===\n",
      "D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset -> OK\n",
      "D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset\\list\\train.txt -> OK\n",
      "D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset\\list\\test.txt -> OK\n",
      "D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset\\list\\groundtruth.txt -> OK\n",
      "\n",
      "[Stage 1] Checking model download/loading ...\n",
      "âœ… Model loaded successfully in 2.9s\n",
      "Hidden size: 1024\n",
      "\n",
      "[Stage 2] Checking dataset reading ...\n",
      "âœ… Data files read successfully.\n",
      "Train lines: 4744, Test lines: 207, Groundtruth lines: 207\n",
      "\n",
      "[Stage 3] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Paths (EDITED) ===\n",
    "DATASET_ROOT = Path(r\"D:\\Swinburne\\Degree3_S2\\COS30082_AML\\GroupAssignment\\dataset\")\n",
    "TRAIN_LIST       = DATASET_ROOT / \"list\" / \"train.txt\"\n",
    "TEST_LIST        = DATASET_ROOT / \"list\" / \"test.txt\"\n",
    "GROUNDTRUTH_LIST = DATASET_ROOT / \"list\" / \"groundtruth.txt\"\n",
    "\n",
    "# === Hyperparameters ===\n",
    "MODEL_NAME = \"facebook/vit-mae-large\"\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.1\n",
    "NUM_WORKERS = 4\n",
    "SEED = 42\n",
    "AMP = True  # mixed precision\n",
    "\n",
    "# === Check paths ===\n",
    "print(\"=== Path Check ===\")\n",
    "for p in [DATASET_ROOT, TRAIN_LIST, TEST_LIST, GROUNDTRUTH_LIST]:\n",
    "    print(f\"{p} -> {'OK' if p.exists() else 'MISSING'}\")\n",
    "\n",
    "# === Stage 1: Check model loading ===\n",
    "print(\"\\n[Stage 1] Checking model download/loading ...\")\n",
    "t0 = time.time()\n",
    "try:\n",
    "    from transformers import AutoImageProcessor, ViTMAEModel\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "    backbone = ViTMAEModel.from_pretrained(MODEL_NAME)\n",
    "    print(f\"âœ… Model loaded successfully in {time.time()-t0:.1f}s\")\n",
    "    print(f\"Hidden size: {backbone.config.hidden_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Model load failed after {time.time()-t0:.1f}s\")\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# === Stage 2: Check data file reading ===\n",
    "print(\"\\n[Stage 2] Checking dataset reading ...\")\n",
    "try:\n",
    "    with open(TRAIN_LIST, \"r\", encoding=\"utf-8\") as f:\n",
    "        train_lines = sum(1 for _ in f)\n",
    "    with open(TEST_LIST, \"r\", encoding=\"utf-8\") as f:\n",
    "        test_lines = sum(1 for _ in f)\n",
    "    with open(GROUNDTRUTH_LIST, \"r\", encoding=\"utf-8\") as f:\n",
    "        gt_lines = sum(1 for _ in f)\n",
    "    print(f\"âœ… Data files read successfully.\")\n",
    "    print(f\"Train lines: {train_lines}, Test lines: {test_lines}, Groundtruth lines: {gt_lines}\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Dataset read failed.\")\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# === Device check ===\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n[Stage 3] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da71d4e",
   "metadata": {},
   "source": [
    "# Data List Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d57144ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files line counts -> train: 4744 | test: 207 | groundtruth: 207\n"
     ]
    }
   ],
   "source": [
    "def read_list_with_labels(list_path: Path) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Read lines of form: 'relative/path label' -> returns [(rel_path, label_int), ...]\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    with open(list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            rel_path, label_str = parts\n",
    "            try:\n",
    "                label = int(label_str)\n",
    "            except:\n",
    "                continue\n",
    "            pairs.append((rel_path, label))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def read_list_paths(list_path: Path) -> List[str]:\n",
    "    \"\"\"\n",
    "    Read lines like: 'relative/path' (no labels).\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    with open(list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 1:\n",
    "                continue\n",
    "            rel_path = parts[0]\n",
    "            paths.append(rel_path)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def build_groundtruth_map(gt_path: Path) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build mapping {rel_path: label} from groundtruth.txt.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            rel_path, label_str = parts\n",
    "            try:\n",
    "                mapping[rel_path] = int(label_str)\n",
    "            except:\n",
    "                continue\n",
    "    return mapping\n",
    "\n",
    "print(\"Files line counts ->\",\n",
    "      \"train:\", sum(1 for _ in open(TRAIN_LIST, \"r\", encoding=\"utf-8\")),\n",
    "      \"| test:\", sum(1 for _ in open(TEST_LIST, \"r\", encoding=\"utf-8\")),\n",
    "      \"| groundtruth:\", sum(1 for _ in open(GROUNDTRUTH_LIST, \"r\", encoding=\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d4518",
   "metadata": {},
   "source": [
    "# Transforms (Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30199241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "# Herbarium: heavy augmentation\n",
    "train_herbarium_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(45),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Photo: light augmentation\n",
    "train_photo_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMAGE_SIZE),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Validation/Test: center-crop evaluation\n",
    "eval_transform = T.Compose([\n",
    "    T.Resize(IMAGE_SIZE + 32),\n",
    "    T.CenterCrop(IMAGE_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def pick_transform(rel_path: str, train: bool = True):\n",
    "    \"\"\"\n",
    "    Heuristically choose transform by path keyword ('herbarium' or 'photo').\n",
    "    Defaults to 'photo' style if not matched.\n",
    "    \"\"\"\n",
    "    low = rel_path.lower()\n",
    "    if \"herbarium\" in low:\n",
    "        return train_herbarium_transform if train else eval_transform\n",
    "    if \"photo\" in low:\n",
    "        return train_photo_transform if train else eval_transform\n",
    "    return train_photo_transform if train else eval_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b6a53",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b40d7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 100\n",
      "Validation pairs: 207, missing or unseen labels: 0\n",
      "Checked 30 samples, missing=0\n"
     ]
    }
   ],
   "source": [
    "class TxtDataset(Dataset):\n",
    "    def __init__(self, root: Path, pairs: List[Tuple[str, int]], train: bool):\n",
    "        self.root = root\n",
    "        self.samples = pairs\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path, label = self.samples[idx]\n",
    "        img_path = self.root / rel_path\n",
    "        with Image.open(img_path).convert(\"RGB\") as img:\n",
    "            transform = pick_transform(rel_path, train=self.train)\n",
    "            img_t = transform(img)\n",
    "        return img_t, label\n",
    "\n",
    "\n",
    "# Build train pairs and remap labels to [0..C-1]\n",
    "train_pairs = read_list_with_labels(TRAIN_LIST)\n",
    "labels = sorted(set(lbl for _, lbl in train_pairs))\n",
    "label_to_idx = {lbl: i for i, lbl in enumerate(labels)}\n",
    "idx_to_label = {i: lbl for lbl, i in label_to_idx.items()}\n",
    "train_pairs_remap = [(p, label_to_idx[lbl]) for p, lbl in train_pairs]\n",
    "num_classes = len(labels)\n",
    "print(f\"Num classes: {num_classes}\")\n",
    "\n",
    "train_ds = TxtDataset(DATASET_ROOT, train_pairs_remap, train=True)\n",
    "\n",
    "# Build validation pairs from test.txt + groundtruth.txt\n",
    "test_paths = read_list_paths(TEST_LIST)\n",
    "gt_map = build_groundtruth_map(GROUNDTRUTH_LIST)\n",
    "\n",
    "val_pairs = []\n",
    "missing = 0\n",
    "for rel in test_paths:\n",
    "    if rel in gt_map and gt_map[rel] in label_to_idx:\n",
    "        val_pairs.append((rel, label_to_idx[gt_map[rel]]))\n",
    "    else:\n",
    "        missing += 1\n",
    "print(f\"Validation pairs: {len(val_pairs)}, missing or unseen labels: {missing}\")\n",
    "\n",
    "val_ds = TxtDataset(DATASET_ROOT, val_pairs, train=False)\n",
    "\n",
    "# Tip: if you see hanging in notebooks on Windows, try NUM_WORKERS = 0\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Quick smoke test: check a few sample paths exist\n",
    "def check_txt_paths(root: Path, pairs: List[Tuple[str,int]], k: int = 20):\n",
    "    missing = 0\n",
    "    for rel, _ in pairs[:k]:\n",
    "        p = root / rel\n",
    "        if not p.exists():\n",
    "            print(\"MISSING:\", p)\n",
    "            missing += 1\n",
    "    print(f\"Checked {min(k, len(pairs))} samples, missing={missing}\")\n",
    "\n",
    "check_txt_paths(DATASET_ROOT, train_pairs, k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa98b57",
   "metadata": {},
   "source": [
    "# Model: MAE Encoder + Linear Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bdbd644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1024, out_features=100, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Keep processor for potential future use; fast preprocessor is fine\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "backbone = ViTMAEModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class MAEClassifier(nn.Module):\n",
    "    def __init__(self, backbone: ViTMAEModel, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        hidden = backbone.config.hidden_size  # 1024 for ViT-L\n",
    "        self.classifier = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(pixel_values=x)\n",
    "        cls = out.last_hidden_state[:, 0]   # [CLS] token\n",
    "        logits = self.classifier(cls)\n",
    "        return logits\n",
    "\n",
    "model = MAEClassifier(backbone, num_classes).to(device)\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0fcf9a",
   "metadata": {},
   "source": [
    "# Optimizer, Scheduler & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6c640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24640\\2077627818.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n"
     ]
    }
   ],
   "source": [
    "# For a linear-probe phase, set requires_grad=False first, then unfreeze later.\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "total_steps = EPOCHS * max(1, len(train_loader))\n",
    "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "# Optional: speed up cudnn on fixed input size\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1cb933",
   "metadata": {},
   "source": [
    "# Training & Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db217077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running smoke test to verify one batch...\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(loader, train=True):\n",
    "    stage = \"TRAIN\" if train else \"VAL\"\n",
    "    print(f\"--> Starting {stage} phase...\")\n",
    "    model.train(train)\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=stage, leave=False, ncols=100)\n",
    "\n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=AMP):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"acc\": f\"{(correct / max(total, 1)):.4f}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / max(total, 1)\n",
    "    acc = correct / max(total, 1)\n",
    "    print(f\"--> Finished {stage}: loss={avg_loss:.4f}, acc={acc:.4f}\")\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "# Optional: smoke test\n",
    "print(\"Running smoke test to verify one batch...\")\n",
    "imgs0, labels0 = next(iter(train_loader))\n",
    "print(\"Smoke batch:\", imgs0.shape, labels0.shape)\n",
    "with torch.no_grad():\n",
    "    _ = model(imgs0.to(device))\n",
    "print(\"Forward pass OK.\")\n",
    "\n",
    "# Training loop with progress prints\n",
    "best_acc = 0.0\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n========== Epoch {ep}/{EPOCHS} ==========\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(\"ðŸš€ Training started...\")\n",
    "    tr_loss, tr_acc = run_one_epoch(train_loader, train=True)\n",
    "\n",
    "    print(\"ðŸ§ª Validation started...\")\n",
    "    va_loss, va_acc = run_one_epoch(val_loader, train=False)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[{ep:02d}/{EPOCHS}] train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.4f} | {dt:.1f}s\")\n",
    "\n",
    "    if va_acc > best_acc:\n",
    "        print(\"ðŸ’¾ Saving new best checkpoint ...\")\n",
    "        best_acc = va_acc\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"label_to_idx\": label_to_idx,\n",
    "            \"idx_to_label\": idx_to_label,\n",
    "            \"config\": {\"model_name\": MODEL_NAME, \"image_size\": IMAGE_SIZE}\n",
    "        }, \"best_mae_classifier.pt\")\n",
    "        print(f\"âœ… New best val acc: {best_acc:.4f} saved successfully!\")\n",
    "\n",
    "print(\"\\nTraining complete ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
